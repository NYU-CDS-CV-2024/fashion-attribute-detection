{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88e9b3f4-302e-48ea-80b7-606232a57b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image, ImageOps\n",
    "import sys\n",
    "import re\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf664e7-d652-4370-9e7f-f48e22c389dc",
   "metadata": {},
   "source": [
    "# Preprocess CSV:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5f101-a86c-468e-9a88-5853dcdbc621",
   "metadata": {},
   "source": [
    "### Fabric types:\n",
    "  0 denim, 1 cotton, 2 leather, 3 furry, 4 knitted, 5 chiffon, 6 other, 7 NA\n",
    "\n",
    "### Pattern types:\n",
    "  0 floral, 1 graphic, 2 striped, 3 pure color, 4 lattice, 5 other, 6 color block, 7 NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29a456d-e6c9-47c0-bd6d-dd7c89aead23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-45718566/ipykernel_4073150/908819537.py:15: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  keypoints_loc = pd.read_csv('/vast/amr10211/cv_data/keypoints/keypoints_loc.txt', delim_whitespace=True, names=keypoint_headers)\n"
     ]
    }
   ],
   "source": [
    "#Fabric Data:\n",
    "fabric_headers = ['img_name', 'upper_fabric', 'lower_fabric', 'outer_fabric']\n",
    "fabric_ann = pd.read_csv('/vast/amr10211/cv_data/labels/texture/fabric_ann.txt', delimiter=' ', names=fabric_headers)\n",
    "\n",
    "#Pattern Data:\n",
    "pattern_headers = ['img_name', 'upper_pattern', 'lower_pattern', 'outer_pattern']\n",
    "pattern_ann = pd.read_csv('/vast/amr10211/cv_data/labels/texture/pattern_ann.txt', delimiter=' ', names=pattern_headers)\n",
    "\n",
    "#Keypoints Data (for filtering for full body images):\n",
    "keypoint_headers = ['img_name', 'x_1', 'y_1', 'x_2', 'y_2', 'x_3', 'y_3', 'x_4', 'y_4', 'x_5', 'y_5',\n",
    "           'x_6', 'y_6', 'x_7', 'y_7', 'x_8', 'y_8', 'x_9', 'y_9', 'x_10', 'y_10',\n",
    "           'x_11', 'y_11', 'x_12', 'y_12', 'x_13', 'y_13', 'x_14', 'y_14', 'x_15', 'y_15',\n",
    "           'x_16', 'y_16', 'x_17', 'y_17', 'x_18', 'y_18', 'x_19', 'y_19', 'x_20', 'y_20',\n",
    "           'x_21', 'y_21']\n",
    "keypoints_loc = pd.read_csv('/vast/amr10211/cv_data/keypoints/keypoints_loc.txt', delim_whitespace=True, names=keypoint_headers)\n",
    "\n",
    "img_names = keypoints_loc['img_name'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eedcec64-6d93-4513-85d5-c5e234ecb787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge and filter Data\n",
    "\n",
    "data_df = pd.merge(fabric_ann,pattern_ann, on='img_name') #Merge fabric and pattern\n",
    "data_df = data_df[data_df['img_name'].isin(img_names)] #Filter for full body images \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29053adc-9bc2-4713-82cc-0ac85d23caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_id(img_name):\n",
    "    # Find the index of the last dash\n",
    "    last_dash_index = img_name.rfind(\"-\")\n",
    "\n",
    "    # Extract the substring before the last dash\n",
    "    substring_before_last_dash = img_name[:last_dash_index]\n",
    "\n",
    "    return substring_before_last_dash\n",
    "\n",
    "data_df['img_id'] = data_df['img_name'].apply(get_image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e019171d-eff7-4797-b0a8-72c8aef7789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_num(img_name):\n",
    "    # Find the index of the last dash\n",
    "    last_dash_index = img_name.rfind(\"-\")\n",
    "\n",
    "    # Find the index of the next underscore after the last dash\n",
    "    next_underscore_index = img_name.find(\"_\", last_dash_index)\n",
    "\n",
    "    # Extract the substring between the last dash and the next underscore\n",
    "    substring_between_dash_and_underscore = img_name[last_dash_index + 1:next_underscore_index]\n",
    "\n",
    "    return substring_between_dash_and_underscore\n",
    "\n",
    "data_df['img_num'] = data_df['img_name'].apply(get_image_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a143587f-bf00-4460-8dff-f1df1f97f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_view(img_name):\n",
    "    # Find the index of the last dash\n",
    "    last_dash_index = img_name.rfind(\"-\")\n",
    "    \n",
    "    # Find the index of the underscore after the last dash\n",
    "    underscore_index = img_name.find(\"_\", last_dash_index)\n",
    "    \n",
    "    # Find the index of the underscore after the one following the last dash\n",
    "    next_underscore_index = img_name.find(\"_\", underscore_index + 1)\n",
    "    \n",
    "    # Find the index of the period\n",
    "    period_index = img_name.rfind(\".\")\n",
    "    \n",
    "    # Extract the substring between the underscore after the last dash and the period\n",
    "    substring_between_underscores_and_period = img_name[next_underscore_index + 1:period_index]\n",
    "    \n",
    "    return substring_between_underscores_and_period\n",
    "\n",
    "    \n",
    "data_df['img_view'] = data_df['img_name'].apply(get_image_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4452718c-dc98-4b09-9573-9771c370fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segm_name(img_name):\n",
    "    segm_name = img_name[:-4]\n",
    "    segm_name = segm_name + '_segm.png'\n",
    "    return segm_name\n",
    "\n",
    "data_df['segm_name'] = data_df['img_name'].apply(get_segm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb9a210a-1797-4834-a09e-77c5f107d05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out images without segmentations:\n",
    "segm_names = os.listdir('/vast/amr10211/cv_data/segm_preprocessed')\n",
    "\n",
    "# Filter out directories, if any\n",
    "segm_names = [segm for segm in segm_names if os.path.isfile(os.path.join('/vast/amr10211/cv_data/segm_preprocessed', segm))]\n",
    "\n",
    "data_df = data_df[data_df['segm_name'].isin(segm_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0260abc0-ff3e-4085-9c4a-feec8fa11d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = ['upper_fabric', 'lower_fabric', 'outer_fabric', 'upper_pattern', 'lower_pattern', 'outer_pattern']\n",
    "data_df = data_df.dropna(subset=columns_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df7212f7-4727-4611-9e63-8d917d04f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv('full_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ab13b-01fb-4c5e-9644-b9ad066dac6c",
   "metadata": {},
   "source": [
    "### Train/Val/Test Split for Segmentation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2695eca0-2ebb-4f78-9784-7459822fc676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#80/10/10 train/val/test split with no data leakage of images with multiple poses\n",
    "\n",
    "group_shuffle_split_80_20 = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=47)\n",
    "group_shuffle_split_50_50 = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=47)\n",
    "\n",
    "for train_index, temp_index in group_shuffle_split_80_20.split(data_df, groups=data_df['img_id']):\n",
    "    train_df = data_df.iloc[train_index]\n",
    "    temp_df = data_df.iloc[temp_index]\n",
    "\n",
    "for val_index, test_index in  group_shuffle_split_50_50.split(temp_df, groups=temp_df['img_id']):\n",
    "    val_df = temp_df.iloc[val_index]\n",
    "    test_df = temp_df.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "681c77e6-b9d7-4b76-9b19-f2b782c67b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching image_ids found between any sets.\n"
     ]
    }
   ],
   "source": [
    "#Check for leakage:\n",
    "train_image_ids_set = set(train_df['img_id'])\n",
    "val_image_ids_set = set(val_df['img_id'])\n",
    "test_image_ids_set = set(test_df['img_id'])\n",
    "\n",
    "intersection_train_val = train_image_ids_set.intersection(val_image_ids_set)\n",
    "intersection_train_test = train_image_ids_set.intersection(test_image_ids_set)\n",
    "intersection_val_test = val_image_ids_set.intersection(test_image_ids_set)\n",
    "\n",
    "# Check if there are any matching image_ids between the sets\n",
    "if intersection_train_val:\n",
    "    print(\"Matching image_ids between train_set and val_set:\", intersection_train_val)\n",
    "\n",
    "if intersection_train_test:\n",
    "    print(\"Matching image_ids between train_set and test_set:\", intersection_train_test)\n",
    "\n",
    "if intersection_val_test:\n",
    "    print(\"Matching image_ids between val_set and test_set:\", intersection_val_test)\n",
    "else:\n",
    "    print(\"No matching image_ids found between any sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7db0249-7ed2-4ff5-aa4c-ebe47b16617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to CSV:\n",
    "train_df.to_csv('segmentation_train.csv', index=False)\n",
    "val_df.to_csv('segmentation_val.csv', index=False)\n",
    "test_df.to_csv('segmentation_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1edaf-3f4a-4149-8e51-4a65a9c28afa",
   "metadata": {},
   "source": [
    "### Preprocessing and Train/Val/Test Split for Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c99ae305-bd69-466c-a9ee-d1d66162a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/vast/amr10211/cv_data/masked_images'\n",
    "masked_image_list = [f for f in os.listdir(img_dir) if os.path.isfile(os.path.join(img_dir, f))]\n",
    "masked_data_df = pd.DataFrame({'masked_img_name':masked_image_list})\n",
    "masked_data_df['segm_name'] = masked_data_df['masked_img_name'].str[:-6] + '.png'\n",
    "\n",
    "data = pd.read_csv( '../data/full_data.csv')\n",
    "data = pd.merge(masked_data_df, data, on ='segm_name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60ed3fc6-59f6-46fc-91f3-2ade6d372dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode Labels based on number before .png\n",
    "# 1 (upper)\n",
    "# 2 (outer)\n",
    "# 3 (lower) \n",
    "# 4 (lower)\n",
    "# 5 (upper)\n",
    "# 6 (upper)\n",
    "\n",
    "data['clothing_type'] = data['masked_img_name'].str[-5].astype(int)\n",
    "\n",
    "def map_fabric_pattern(data):\n",
    "    if int(data['clothing_type']) in (1,5,6):\n",
    "        data['fabric'] = data['upper_fabric']\n",
    "        data['pattern'] = data['upper_pattern']\n",
    "    elif int(data['clothing_type']) in (3,4):\n",
    "        data['fabric'] = data['lower_fabric']\n",
    "        data['pattern'] = data['lower_pattern']\n",
    "    elif int(data['clothing_type'])== 2:\n",
    "        data['fabric'] = data['outer_fabric']\n",
    "        data['pattern'] = data['outer_pattern']\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = data.apply(map_fabric_pattern, axis=1)\n",
    "\n",
    "data = data[['masked_img_name', 'clothing_type', 'fabric', 'pattern', 'segm_name', 'img_id']].dropna()\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f3a3318-58db-4b76-ae92-977e5ce74c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fabric types:\n",
    "# 0 denim, 1 cotton, 2 leather, 3 furry, 4 knitted, 5 chiffon, 6 other, 7 NA\n",
    "fabrics = [0,1,2,4,5,7]\n",
    "data = data[data['fabric'].isin(fabrics)]\n",
    "\n",
    "def map_labels(label):\n",
    "    mapping = {\n",
    "        0: 0,\n",
    "        1: 1,\n",
    "        2: 2,\n",
    "        4: 3,\n",
    "        5: 4,\n",
    "        7: 5,\n",
    "    }\n",
    "    return mapping.get(label, label)\n",
    "\n",
    "data['fabric'] = data['fabric'].apply(map_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97540bbc-d808-405e-bbff-6064d8c8720c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masked_img_name</th>\n",
       "      <th>clothing_type</th>\n",
       "      <th>fabric</th>\n",
       "      <th>pattern</th>\n",
       "      <th>segm_name</th>\n",
       "      <th>img_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MEN-Denim-id_00000080-01_7_additional_segm_1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>MEN-Denim-id_00000080-01_7_additional_segm.png</td>\n",
       "      <td>MEN-Denim-id_00000080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MEN-Denim-id_00000080-01_7_additional_segm_4.png</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>MEN-Denim-id_00000080-01_7_additional_segm.png</td>\n",
       "      <td>MEN-Denim-id_00000080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEN-Denim-id_00000089-01_7_additional_segm_1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>MEN-Denim-id_00000089-01_7_additional_segm.png</td>\n",
       "      <td>MEN-Denim-id_00000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MEN-Denim-id_00000089-01_7_additional_segm_4.png</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>MEN-Denim-id_00000089-01_7_additional_segm.png</td>\n",
       "      <td>MEN-Denim-id_00000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MEN-Denim-id_00000089-02_7_additional_segm_1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>MEN-Denim-id_00000089-02_7_additional_segm.png</td>\n",
       "      <td>MEN-Denim-id_00000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25076</th>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007970-01_7_additional_s...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007970-01_7_additional_s...</td>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25077</th>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007976-01_4_full_segm_1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007976-01_4_full_segm.png</td>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25078</th>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007976-01_4_full_segm_4.png</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007976-01_4_full_segm.png</td>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25079</th>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007979-03_4_full_segm_6.png</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007979-03_4_full_segm.png</td>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25080</th>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007979-04_4_full_segm_6.png</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007979-04_4_full_segm.png</td>\n",
       "      <td>WOMEN-Tees_Tanks-id_00007979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24856 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         masked_img_name  clothing_type  \\\n",
       "0       MEN-Denim-id_00000080-01_7_additional_segm_1.png              1   \n",
       "1       MEN-Denim-id_00000080-01_7_additional_segm_4.png              4   \n",
       "2       MEN-Denim-id_00000089-01_7_additional_segm_1.png              1   \n",
       "3       MEN-Denim-id_00000089-01_7_additional_segm_4.png              4   \n",
       "4       MEN-Denim-id_00000089-02_7_additional_segm_1.png              1   \n",
       "...                                                  ...            ...   \n",
       "25076  WOMEN-Tees_Tanks-id_00007970-01_7_additional_s...              4   \n",
       "25077  WOMEN-Tees_Tanks-id_00007976-01_4_full_segm_1.png              1   \n",
       "25078  WOMEN-Tees_Tanks-id_00007976-01_4_full_segm_4.png              4   \n",
       "25079  WOMEN-Tees_Tanks-id_00007979-03_4_full_segm_6.png              6   \n",
       "25080  WOMEN-Tees_Tanks-id_00007979-04_4_full_segm_6.png              6   \n",
       "\n",
       "       fabric  pattern                                          segm_name  \\\n",
       "0           1        3     MEN-Denim-id_00000080-01_7_additional_segm.png   \n",
       "1           1        4     MEN-Denim-id_00000080-01_7_additional_segm.png   \n",
       "2           1        3     MEN-Denim-id_00000089-01_7_additional_segm.png   \n",
       "3           1        3     MEN-Denim-id_00000089-01_7_additional_segm.png   \n",
       "4           1        2     MEN-Denim-id_00000089-02_7_additional_segm.png   \n",
       "...       ...      ...                                                ...   \n",
       "25076       0        3  WOMEN-Tees_Tanks-id_00007970-01_7_additional_s...   \n",
       "25077       1        6    WOMEN-Tees_Tanks-id_00007976-01_4_full_segm.png   \n",
       "25078       0        3    WOMEN-Tees_Tanks-id_00007976-01_4_full_segm.png   \n",
       "25079       1        3    WOMEN-Tees_Tanks-id_00007979-03_4_full_segm.png   \n",
       "25080       1        3    WOMEN-Tees_Tanks-id_00007979-04_4_full_segm.png   \n",
       "\n",
       "                             img_id  \n",
       "0             MEN-Denim-id_00000080  \n",
       "1             MEN-Denim-id_00000080  \n",
       "2             MEN-Denim-id_00000089  \n",
       "3             MEN-Denim-id_00000089  \n",
       "4             MEN-Denim-id_00000089  \n",
       "...                             ...  \n",
       "25076  WOMEN-Tees_Tanks-id_00007970  \n",
       "25077  WOMEN-Tees_Tanks-id_00007976  \n",
       "25078  WOMEN-Tees_Tanks-id_00007976  \n",
       "25079  WOMEN-Tees_Tanks-id_00007979  \n",
       "25080  WOMEN-Tees_Tanks-id_00007979  \n",
       "\n",
       "[24856 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efe412e1-2de0-462d-8bb4-7d4051037688",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('full_data_classification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ee0008-3b9f-4e52-997e-9efb4e5f32a7",
   "metadata": {},
   "source": [
    "# Split CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "576613e9-371a-4d71-8d81-3f9f070ddc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#80/10/10 train/val/test split with no data leakage of images with multiple poses\n",
    "\n",
    "group_shuffle_split_80_20 = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=47)\n",
    "group_shuffle_split_50_50 = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=47)\n",
    "\n",
    "for train_index, temp_index in group_shuffle_split_80_20.split(data, groups=data['img_id']):\n",
    "    train_df = data.iloc[train_index]\n",
    "    temp_df = data.iloc[temp_index]\n",
    "\n",
    "for val_index, test_index in  group_shuffle_split_50_50.split(temp_df, groups=temp_df['img_id']):\n",
    "    val_df = temp_df.iloc[val_index]\n",
    "    test_df = temp_df.iloc[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96fe2a7c-d0ba-468e-934f-becd24436281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching image_ids found between any sets.\n"
     ]
    }
   ],
   "source": [
    "#Check for leakage:\n",
    "train_image_ids_set = set(train_df['img_id'])\n",
    "val_image_ids_set = set(val_df['img_id'])\n",
    "test_image_ids_set = set(test_df['img_id'])\n",
    "\n",
    "intersection_train_val = train_image_ids_set.intersection(val_image_ids_set)\n",
    "intersection_train_test = train_image_ids_set.intersection(test_image_ids_set)\n",
    "intersection_val_test = val_image_ids_set.intersection(test_image_ids_set)\n",
    "\n",
    "# Check if there are any matching image_ids between the sets\n",
    "if intersection_train_val:\n",
    "    print(\"Matching image_ids between train_set and val_set:\", intersection_train_val)\n",
    "\n",
    "if intersection_train_test:\n",
    "    print(\"Matching image_ids between train_set and test_set:\", intersection_train_test)\n",
    "\n",
    "if intersection_val_test:\n",
    "    print(\"Matching image_ids between val_set and test_set:\", intersection_val_test)\n",
    "else:\n",
    "    print(\"No matching image_ids found between any sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a2f884a-b8d2-46bd-9fc6-b015cc380339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to CSV:\n",
    "train_df.to_csv('classification_train.csv', index=False)\n",
    "val_df.to_csv('classification_val.csv', index=False)\n",
    "test_df.to_csv('classification_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "305a2236-35cc-458e-a395-6b9c889baa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-45718566/ipykernel_4073150/2984432689.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_sample = train_df.groupby('pattern', group_keys=False).apply(lambda x: x.sample(frac=0.025, random_state=47)).reset_index(drop=True)\n",
      "/state/partition1/job-45718566/ipykernel_4073150/2984432689.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_sample = val_df.groupby('pattern', group_keys=False).apply(lambda x: x.sample(frac=0.025, random_state=47)).reset_index(drop=True)\n",
      "/state/partition1/job-45718566/ipykernel_4073150/2984432689.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_sample = test_df.groupby('pattern', group_keys=False).apply(lambda x: x.sample(frac=0.025, random_state=47)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "#Make small datasources for model buidling:\n",
    "train_sample = train_df.groupby('pattern', group_keys=False).apply(lambda x: x.sample(frac=0.025, random_state=47)).reset_index(drop=True)\n",
    "val_sample = val_df.groupby('pattern', group_keys=False).apply(lambda x: x.sample(frac=0.025, random_state=47)).reset_index(drop=True)\n",
    "test_sample = test_df.groupby('pattern', group_keys=False).apply(lambda x: x.sample(frac=0.025, random_state=47)).reset_index(drop=True)\n",
    "\n",
    "train_sample.to_csv('train_sample_pattern.csv', index=False)\n",
    "val_sample.to_csv('val_sample_pattern.csv', index=False)\n",
    "test_sample.to_csv('test_sample_pattern.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "068f5068-192e-428a-b0ea-da73319a6a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-45718566/ipykernel_4073150/2992081575.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_sample = train_df.groupby('fabric', group_keys=False).apply(lambda x: x.sample(frac=0.025, random_state=47)).reset_index(drop=True)\n",
      "/state/partition1/job-45718566/ipykernel_4073150/2992081575.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_sample = val_df.groupby('fabric', group_keys=False).apply(lambda x: x.sample(frac=0.025, random_state=47)).reset_index(drop=True)\n",
      "/state/partition1/job-45718566/ipykernel_4073150/2992081575.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_sample = test_df.groupby('fabric', group_keys=False).apply(lambda x: x.sample(frac=0.025, random_state=47)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "#Make small datasources for model buidling:\n",
    "train_sample = train_df.groupby('fabric', group_keys=False).apply(lambda x: x.sample(frac=0.025, random_state=47)).reset_index(drop=True)\n",
    "val_sample = val_df.groupby('fabric', group_keys=False).apply(lambda x: x.sample(frac=0.025, random_state=47)).reset_index(drop=True)\n",
    "test_sample = test_df.groupby('fabric', group_keys=False).apply(lambda x: x.sample(frac=0.025, random_state=47)).reset_index(drop=True)\n",
    "\n",
    "train_sample.to_csv('train_sample_fabric.csv', index=False)\n",
    "val_sample.to_csv('val_sample_fabric.csv', index=False)\n",
    "test_sample.to_csv('test_sample_fabric.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbf6848-8622-490e-a90e-73d4741d705e",
   "metadata": {},
   "source": [
    "## Preprocess Image Segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a0123b6-ed9c-4b0d-afc9-2be97f726454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting and mapping PNG files: 100%|██████████| 12701/12701 [23:25<00:00,  9.04it/s]\n"
     ]
    }
   ],
   "source": [
    "mapping = {\n",
    "    1: 1,\n",
    "    2: 2,\n",
    "    3: 3,\n",
    "    5: 4,\n",
    "    6: 4,\n",
    "    4: 5,\n",
    "    21: 6\n",
    "}\n",
    "\n",
    "input_directory = '/vast/amr10211/cv_data/segm'\n",
    "output_directory = '/vast/amr10211/cv_data/segm_preprocessed'\n",
    "png_files = [filename for filename in os.listdir(input_directory) if filename.endswith('.png')]\n",
    "\n",
    "for filename in tqdm(png_files, desc=\"Converting and mapping PNG files\"):\n",
    "    input_filepath = os.path.join(input_directory, filename)\n",
    "    output_filepath = os.path.join(output_directory, filename)\n",
    "    \n",
    "    # Open the image\n",
    "    img = Image.open(input_filepath)\n",
    "    \n",
    "    # Convert the image to a NumPy array\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Apply the mapping to each pixel value\n",
    "    mapped_array = np.vectorize(lambda x: mapping.get(x, 0))(img_array)\n",
    "    \n",
    "    # Convert the mapped array back to an image\n",
    "    mapped_img = Image.fromarray(mapped_array.astype(np.uint8))\n",
    "    \n",
    "    # Save the mapped image as PNG\n",
    "    mapped_img.save(output_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf0597e4-dc6c-4f7e-87ea-03098867e184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting and mapping PNG files: 100%|██████████| 12701/12701 [02:56<00:00, 71.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 0, Count: 9088520009\n",
      "Value: 1, Count: 400516322\n",
      "Value: 4, Count: 399366922\n",
      "Value: 2, Count: 191766313\n",
      "Value: 6, Count: 70451055\n",
      "Value: 5, Count: 277766047\n",
      "Value: 3, Count: 52070582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_directory = '/vast/amr10211/cv_data/segm_preprocessed'\n",
    "png_files = [filename for filename in os.listdir(input_directory) if filename.endswith('.png')]\n",
    "\n",
    "unique_value_counts = {}\n",
    "\n",
    "for filename in tqdm(png_files, desc=\"Converting and mapping PNG files\"):\n",
    "    input_filepath = os.path.join(input_directory, filename)\n",
    "    \n",
    "    # Open the image\n",
    "    img = Image.open(input_filepath)\n",
    "    \n",
    "    # Convert the image to a NumPy array\n",
    "    img_array = np.array(img)\n",
    "\n",
    "    # Count unique values in the array\n",
    "    unique_values, counts = np.unique(img_array, return_counts=True)\n",
    "    \n",
    "    # Update the counts in the dictionary\n",
    "    for value, count in zip(unique_values, counts):\n",
    "        if value not in unique_value_counts:\n",
    "            unique_value_counts[value] = 0\n",
    "        unique_value_counts[value] += count\n",
    "\n",
    "# Print the final count per unique value\n",
    "for value, count in unique_value_counts.items():\n",
    "    print(f\"Value: {value}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f6ea73-4f85-4fdb-a4b0-f1ea212948fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLM_env",
   "language": "python",
   "name": "dlm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
